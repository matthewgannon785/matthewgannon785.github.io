setwd("C:/Users/matth/Documents/Machine Learning & AI/Assignment2")
load("C:/Users/matth/Documents/Machine Learning & AI/Assignment2/data_epileptic.RData")
library(keras)
library(tfruns)
library(reticulate)
library(jsonlite)
use_condaenv("r-tensorflow")

# Exercise 1.
# Part 1.
# Part 2.
Input_obs <- c(-0.5,0.3)
Input_vec <- matrix(c(-0.5,0.4,-0.2, -0.3,-0.1,0.4), nrow = 2, ncol = 3, byrow = TRUE, dimnames = list(c("row1", "row2"), c("C.1", "C.2", "C.3")))
Input_vec[1,]
H1_vec <- matrix(c(0.3,-0.7,1.3, 0.5,-0.8,1.2), nrow = 3, ncol = 2, byrow = TRUE, 
dimnames = list(c("row1", "row2","row3"), c("C.1", "C.2")))
H2_vec <- c(2, 1.1)

b1 <- c(1.1,-0.8,1.3)
b2 <- c(0.5,-0.8)
b3 <- 8

out_1 <- Input_obs[1]*Input_vec[1,]
out_2 <- Input_obs[2]*Input_vec[2,]

HL1 <- NULL
for(i in 1:3){
HL1[i] <- out_1[i] + out_2[i]+b1[i]
}

sigmoid <- function(x) {
  sigma <- 1/(1+exp(-x))
  return(sigma)
}

input_HL1 <- sigmoid(HL1)
out_3 <- input_HL1[1]*H1_vec[1,]
out_4 <- input_HL1[2]*H1_vec[2,]
out_5 <- input_HL1[3]*H1_vec[3,]

HL2 <- NULL
for(i in 1:2){
HL2[i] <- out_3[i] + out_4[i]+ out_5[i]+b2[i]
}

Relu <- function(x) {
n <- length(x)
sigma <- NULL
for(i in 1:n){
  sigma[i] <- max(x[i],0)
}
  return(sigma)
}

input_HL2 <- Relu(HL2)
out_6 <- input_HL2[1]*H2_vec[1]
out_7 <- input_HL2[2]*H2_vec[2]

OL <- out_6 + out_7 + b3

# Part 3.
target <- 7
Squared loss:

Squared_loss <- function(x,y) {
n <- length(x)
loss <- NULL
for(i in 1:n){
  loss[i] <- (y[i] - x[i])^2
  sum <- sum(loss)
}
  return(sum)
}

Squared_loss(OL, target)

# Exercise 2.
# Part 1.
2048

# Part 2.
I believe 544x100 = 54,400.

# Part 3.
# Part 4.
Penalty Based Regularisation.

is.na(x)
# Exercise 3.
# Part 1.

x <- scale(x)
x_test <- scale(x_test)
y <- to_categorical(y)
y_test <- to_categorical(y_test)

set.seed(1456)
val <- sample(1:nrow(x_test), 690) 
test <- setdiff(1:nrow(x_test), val)
x_val <- x_test[val,]
y_val <- y_test[val,]
x_test <- x_test[test,]
y_test <- y_test[test,]

# need these later
N <- nrow(x)
V <- ncol(x)

dropout_set <- c(0, 0.3, 0.4, 0.5)
lambda_set <- c(0, exp( seq(-6, -4, length = 9) ))
lr_set <- c(0.001, 0.002, 0.005, 0.01)
bs_set <- c(0.005, 0.01, 0.02, 0.03)*N

# NOTE : this will require some time to run

runs_model1 <- tuning_run("model1.R",
runs_dir = "runs_model1",
flags = list(
dropout = dropout_set,
lambda = lambda_set,
lr = lr_set,
bs = bs_set
),
sample = 0.1)

runs_model2 <- tuning_run("model2.R",
runs_dir = "runs_model2",
flags = list(
dropout = dropout_set,
lambda = lambda_set,
lr = lr_set,
bs = bs_set
),
sample = 0.1)

read_metrics <- function(path, files = NULL)
# 'path' is where the runs are --> e.g. "path/to/runs"
{
path <- paste0(path, "/")
if ( is.null(files) ) files <- list.files(path)
n <- length(files)
out <- vector("list", n)
for ( i in 1:n ) {
dir <- paste0(path, files[i], "/tfruns.d/")
out[[i]] <- jsonlite::fromJSON(paste0(dir, "metrics.json"))
out[[i]]$flags <- jsonlite::fromJSON(paste0(dir, "flags.json"))
out[[i]]$evaluation <- jsonlite::fromJSON(paste0(dir, "evaluation.json"))
}
return(out)
}

# to add a smooth line to points
smooth_line <- function(y, span = 0.3) {
x <- 1:length(y)
out <- predict( loess(y ~ x, span = span) )
return(out)
}

# extract results from folders
out_m1 <- read_metrics("runs_model1")
out_m2 <- read_metrics("runs_model2")

# extract training and validation scores
train_acc_m1 <- sapply(out_m1, "[[", "accuracy")
val_acc_m1 <- sapply(out_m1, "[[", "val_accuracy")
train_loss_m1 <- sapply(out_m1, "[[", "loss")
val_loss_m1 <- sapply(out_m1, "[[", "val_loss")

train_acc_m2 <- sapply(out_m2, "[[", "accuracy")
val_acc_m2 <- sapply(out_m2, "[[", "val_accuracy")
train_loss_m2 <- sapply(out_m2, "[[", "loss")
val_loss_m2 <- sapply(out_m2, "[[", "val_loss")


# select the top 10 runs by validation accuracy
sel <- 10
top_m1 <- order(apply(val_acc_m1, 2, max, na.rm = TRUE), decreasing = TRUE)[1:sel]
top_m2 <- order(apply(val_acc_m2, 2, max, na.rm = TRUE), decreasing = TRUE)[1:sel]

# plot loss curves to inspect training process and underfitting/overfitting
cols <- rep(c("black", "dodgerblue3"), each = sel)
out_loss_m1 <- cbind(train_loss_m1[,top_m1], val_loss_m1[,top_m1])
matplot(out_loss_m1, pch = 19, ylab = "Loss", xlab = "Epochs",
col = adjustcolor(cols, 0.2), ylim = c(0, 3))
grid()
tmp_m1 <- apply(out_loss_m1, 2, smooth_line, span = 0.5)
tmp_m1 <- sapply(tmp_m1, "length<-", 100 ) 
# set default length of 100 epochs
matlines(tmp_m1, lty = 1, col = cols, lwd = 2)
legend("topright", legend = c("Training", "Validation"),
fill = unique(cols), bty = "n")

out_loss_m2 <- cbind(train_loss_m2[,top_m2], val_loss_m2[,top_m2])
matplot(out_loss_m2, pch = 19, ylab = "Loss", xlab = "Epochs",
col = adjustcolor(cols, 0.2), ylim = c(0, 3))
grid()
tmp_m2 <- apply(out_loss_m2, 2, smooth_line, span = 0.5)
tmp_m2 <- sapply(tmp_m2, "length<-", 100 ) 
# set default length of 100 epochs
matlines(tmp_m2, lty = 1, col = cols, lwd = 2)
legend("topright", legend = c("Training", "Validation"),
fill = unique(cols), bty = "n")

# plot accuracy curves to inspect performance and overfitting
out_acc_m1 <- cbind(train_acc_m1[,top_m1], val_acc_m1[,top_m1])
matplot(out_acc_m1, pch = 19, ylab = "Accuracy", xlab = "Epochs",
col = adjustcolor(cols, 0.2), ylim = c(0, 1))
grid()
tmp_m1 <- apply(out_acc_m1, 2, smooth_line)
tmp_m1 <- sapply(tmp_m1, "length<-", 100 )
matlines(tmp_m1, lty = 1, col = cols, lwd = 2)
legend("bottomright", legend = c("Training", "Validation"),
fill = unique(cols), bty = "n")

out_acc_m2 <- cbind(train_acc_m2[,top_m2], val_acc_m2[,top_m2])
matplot(out_acc_m2, pch = 19, ylab = "Accuracy", xlab = "Epochs",
col = adjustcolor(cols, 0.2), ylim = c(0, 1))
grid()
tmp_m2 <- apply(out_acc_m2, 2, smooth_line)
tmp_m2 <- sapply(tmp_m2, "length<-", 100 )
matlines(tmp_m2, lty = 1, col = cols, lwd = 2)
legend("bottomright", legend = c("Training", "Validation"),
fill = unique(cols), bty = "n")

res_m1 <- ls_runs(metric_val_accuracy > 0.5,
runs_dir = "runs_model1", order = metric_val_accuracy)

res_m2 <- ls_runs(metric_val_accuracy > 0.5,
runs_dir = "runs_model2", order = metric_val_accuracy)

colu_m1 <- c("metric_val_accuracy", grep("flag", colnames(res_m1), value = TRUE), "epochs_completed")
res_m1[1:5,colu_m1]

colu_m2 <- c("metric_val_accuracy", grep("flag", colnames(res_m2), value = TRUE), "epochs_completed")
res_m2[1:5,colu_m2]






# Part 2.
# Part 3.

# deploy model using optimal hyperparameters
model2 <- keras_model_sequential() %>%
  layer_dense(units = 128, input_shape = V, activation = "relu", name = "layer_1") %>%
  layer_dropout(rate = res_m2$flag_dropout[1]) %>%
  layer_dense(units = 32, activation = "sigmoid", name = "layer_2") %>%
  layer_dropout(rate = res_m2$flag_dropout[1]) %>%
  layer_dense(units = ncol(y), activation = "softmax", name = "layer_out") %>%
  compile(loss = "categorical_crossentropy", metrics = "accuracy",
optimizer = optimizer_adam(learning_rate = res_m2$flag_lr[1]),
)
# merge train and validation data
x_ <- rbind(x, x_val)
y_ <- rbind(y, y_val)
# fit on full data and test on test data
fit <- model2 %>% fit(
x = x_, y = y_,
validation_data = list(x_test, y_test),
epochs = 100,
batch_size = res_m2$flag_bs[1],
verbose = 1,
callbacks = callback_early_stopping(monitor = "val_accuracy", patience = 30)
)

# estimated classes and actual digits
class_hat <- model2 %>% predict(x_test) %>% max.col() - 1
y_labels_test <- max.col(y_test) - 1
# check performance and class-specific performance
tab <- table(y_labels_test, class_hat)
acc <- diag(tab)/rowSums(tab)
cbind(tab, acc)

# deploy model using optimal hyperparameters
model1 <- keras_model_sequential() %>%
   layer_dense(units = 128, input_shape = V, activation = "sigmoid", name = "layer_1") %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = 64, activation = "relu", name = "layer_2") %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = 32, activation = "relu", name = "layer_3",) %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = ncol(y), activation = "softmax", name = "layer_out") %>%
  compile(loss = "categorical_crossentropy", metrics = "accuracy",
optimizer = optimizer_adam(learning_rate = res_m1$flag_lr[1]),
)
# merge train and validation data
x_ <- rbind(x, x_val)
y_ <- rbind(y, y_val)
# fit on full data and test on test data
fit <- model1 %>% fit(
x = x_, y = y_,
validation_data = list(x_test, y_test),
epochs = 100,
batch_size = res_m1$flag_bs[1],
verbose = 1,
callbacks = callback_early_stopping(monitor = "val_accuracy", patience = 30)
)

# estimated classes and actual digits
class_hat <- model1 %>% predict(x_test) %>% max.col() - 1
y_labels_test <- max.col(y_test) - 1
# check performance and class-specific performance
tab <- table(y_labels_test, class_hat)
acc <- diag(tab)/rowSums(tab)
cbind(tab, acc)



# deploy model using optimal hyperparameters
model1 <- keras_model_sequential() %>%
   layer_dense(units = 128, input_shape = V, activation = "sigmoid", name = "layer_1") %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = 64, activation = "relu", name = "layer_2") %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = 32, activation = "relu", name = "layer_3",) %>%
  layer_dropout(rate = res_m1$flag_dropout[1]) %>%
  layer_dense(units = ncol(y), activation = "softmax", name = "layer_out") %>%
  compile(loss = "categorical_crossentropy", metrics = "accuracy",
optimizer = optimizer_adam(learning_rate = res_m1$flag_lr[1]),
)
# merge train and validation data
x_ <- rbind(x, x_val)
y_ <- rbind(y, y_val)
# fit on full data and test on test data
fit <- model1 %>% fit(
x = x_, y = y_,
validation_data = list(x_test, y_test),
epochs = 100,
batch_size = res_m1$flag_bs[1],
verbose = 1,
callbacks = callback_early_stopping(monitor = "val_accuracy", patience = 30)
)



# estimated classes and actual digits
class_hat <- model1 %>% predict(x_test) %>% max.col() - 1
y_labels_test <- max.col(y_test) - 1
# check performance and class-specific performance
tab <- table(y_labels_test, class_hat)
acc <- diag(tab)/rowSums(tab)
cbind(tab, acc)



# deploy model using optimal hyperparameters
model2 <- keras_model_sequential() %>%
  layer_dense(units = 128, input_shape = V, activation = "relu", name = "layer_1") %>%
  layer_dropout(rate = res_m2$flag_dropout[1]) %>%
  layer_dense(units = 32, activation = "sigmoid", name = "layer_2") %>%
  layer_dropout(rate = res_m2$flag_dropout[1]) %>%
  layer_dense(units = ncol(y), activation = "softmax", name = "layer_out") %>%
  compile(loss = "categorical_crossentropy", metrics = "accuracy",
optimizer = optimizer_adam(learning_rate = res_m2$flag_lr[1]),
)
# merge train and validation data
x_ <- rbind(x, x_val)
y_ <- rbind(y, y_val)
# fit on full data and test on test data
fit <- model2 %>% fit(
x = x_, y = y_,
validation_data = list(x_test, y_test),
epochs = 100,
batch_size = res_m2$flag_bs[1],
verbose = 1,
callbacks = callback_early_stopping(monitor = "val_accuracy", patience = 30)
)



# estimated classes and actual digits
class_hat <- model2 %>% predict(x_test) %>% max.col() - 1
y_labels_test <- max.col(y_test) - 1
# check performance and class-specific performance
tab <- table(y_labels_test, class_hat)
acc <- diag(tab)/rowSums(tab)
cbind(tab, acc)



